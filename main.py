import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
# from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.schema.runnable import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from docx import Document as DocxDocument  # Rename to avoid conflicts
from langchain.schema import Document

from langchain.prompts import ChatPromptTemplate


load_dotenv()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("Missing Google API Key. Please check your .env file.")

OPENAI_API_KEY = os.getenv("OPENAI_KEY")
if not OPENAI_API_KEY:
    raise ValueError("Missing OpenAI API Key. Please check your .env file.")



DOC_FOLDER = "data/pdf_files"
CHROMA_PATH = "vector_db_pdf"
MODEL_NAME = "gpt-4o-mini"
TEMPERATURE = 0.7
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200


def load_docx_documents(folder_path):
    docx_docs = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".docx"):
            file_path = os.path.join(folder_path, filename)
            try:
                doc = DocxDocument(file_path)
                doc_content = "\n".join([para.text for para in doc.paragraphs])
                docx_docs.append(Document(page_content=doc_content, metadata={"source": filename}))
            except Exception as e:
                print(f"Error loading {filename}: {e}")
    return docx_docs

docx_docs = load_docx_documents(DOC_FOLDER)
if not docx_docs or not all(isinstance(doc, Document) for doc in docx_docs):
    raise ValueError("No valid LangChain Document objects found in the folder.")


splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP,
    separators=["\n\n", "\n", " ", ""]
)
chunks = splitter.split_documents(docx_docs)
print(f"Total chunks created: {len(chunks)}")




# embedding_model = OpenAIEmbeddings()
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
vector_db = Chroma.from_documents(
    chunks, embeddings, persist_directory=CHROMA_PATH
)
print("Vector DB created successfully.")

retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={"k": 3})

def query_documents(query):
    retrieved_docs = retriever.invoke(query)
    final_docs = retrieved_docs[:3]
    context = "\n".join([doc.page_content for doc in final_docs])[:10000]
    return context



SYSTEM_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£ üéì 
- ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ  
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö ‡πÅ‡∏ô‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß  
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤  
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢** ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥  
- ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÑ‡∏î‡πâ  
- ‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÇ‡∏î‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ **‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠** ‡πÅ‡∏•‡∏∞ **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö**
- ‡∏ï‡∏≠‡∏ô‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô
"""


chat_prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("human", "{query}")  # Placeholder for customer questions
])


llm = ChatOpenAI(
    model_name=MODEL_NAME, 
    temperature=TEMPERATURE, 
    api_key=OPENAI_API_KEY,
    )

retrieval_chain = (
    RunnableParallel(
        {"context": retriever, "query": RunnablePassthrough()}
    )
    | chat_prompt
    | llm
    | StrOutputParser()
)



# llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=TEMPERATURE)
# qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)


query = "‡∏°‡∏µ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏´‡∏°"
context = query_documents(query)
print("Context:\n", context)
response = retrieval_chain.invoke(f"{query}\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n{context}")
# response = retrieval_chain.invoke(query)

print("Response:\n", response)